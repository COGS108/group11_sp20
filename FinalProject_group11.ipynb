{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of contributions towards job automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that PIDs will be scraped from the public submission, but student names will be included.)\n",
    "\n",
    "* [  ] YES - make available\n",
    "* [  ] NO - keep private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your overview here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Michael Baluja\n",
    "- Griffin Mittleman\n",
    "- Deepkiran Sangha\n",
    "- Hannah Williams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members IDs\n",
    "\n",
    "- A14499660\n",
    "- A15734955\n",
    "- A15096006\n",
    "- A16112910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the further integration of job automation into the United States’ workforce related to employment salaries, skills, and employment rates across various job industries in the past 25 years?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your background and prior work here* \n",
    "\n",
    "References (include links):\n",
    "- 1)\n",
    "- 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar prior work we have found looks at the Likelihood of Automation with comparisons between Occupation and Salary. The analyses done in this research use the same automation probability dataset that we work with in our analysis (Dataset 1). The second dataset used in this study focuses on a handful of data categories regarding income levels, which we have additionally adopted into our data collection for this project (Dataset 2). In that manner, the aspect of our study considering income and state automation likelihood acts as an extension of the prior work.\n",
    "\n",
    "References:\n",
    "- 1) https://data.world/quanticdata/occupation-and-salary-by-state-and-likelihood-of-automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize:\n",
    "- Occupations with higher salaries will have a lower probability of being automated.\n",
    "- Jobs with high automation probabilities will show a decrease in employment rates for a period of time surrounding the point at which the automation likelihood was estimated. \n",
    "- High perception and manipulation skills correlate more to a high probability of automation\n",
    "- High creative and social intelligence scores correlate more with a low probability of automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline: (Delete before final submission!)\n",
    "*Fill in your dataset information here*\n",
    "\n",
    "(Copy this information for each dataset)\n",
    "- Dataset Name:\n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets.\n",
    "\n",
    "Dataset 1:\n",
    "- Dataset Name: Occupations by State and Likelihood of Automation\n",
    "- Link to the dataset: https://data.world/wnedds/occupations-by-state-and-likelihood-of-automation\n",
    "- Number of observations: 702 (1 per occupation)\n",
    "\n",
    "This dataset includes job title, OCC code (used to categorize), probability of occupation, and number of employees per state\n",
    "\n",
    "Dataset 2:\n",
    "- Dataset Name: Wage by Occupation\n",
    "- Link to the dataset: https://data.world/quanticdata/occupation-and-salary-by-state-and-likelihood-of-automation/workspace/file?filename=national_M2016_dl.xlsx\n",
    "- Number of observations: 1394\n",
    "\n",
    "This dataset includes job titles, OCC code, and various wage metrics per occupation (salary vs hourly, average pay, statistics for these fields, North American Industry Classification System code, employee number data, etc)\n",
    "\n",
    "Dataset 3:\n",
    "- Dataset Name: Employment by State\n",
    "- Link to the dataset: https://www.bea.gov/data/employment/employment-by-state\n",
    "- Number of observations: 51 (states + DC)\n",
    "\n",
    "This dataset includes the number of employees present per state in any given year.\n",
    "\n",
    "Dataset 4:\n",
    "- Dataset Name: Employed persons by detailed occupation and age (table 11b)\n",
    "- Link to the dataset:  https://www.bls.gov/cps/tables.htm\n",
    "- Number of observations: 567\n",
    "\n",
    "These datasets (collected for years between 2013 and 2019) show the number of employees per occupation by age group, and gives the total number of employees per occupation.\n",
    "\n",
    "Datasets 1,2 provide information by occupations will be combined by their OCC code, since it is the most standard metric they share (note: some datasets have different occupation name string formatting, so this is not as easily used). Since we mostly want to consider this data in terms of probability of automation, data from Dataset 2 will only be kept if there is a corresponding OCC code in Dataset 1. Dataset 3 is used to turn the number of employees per occupation per state in Dataset 1 into fractions so we can look at relative numbers of employement as opposed to absolute numbers. \n",
    "\n",
    "The 7 datasets from Dataset 4 (corresponding to 2013-2019 data) will be combined in order to calculate the change in employment over the 7 year period. This data will be combined by Dataset 1 by OCC code in order to investigate the relationship between job field growth and likelihood of automation for that job field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\ksang\\anaconda3\\lib\\site-packages (4.8.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\ksang\\anaconda3\\lib\\site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: six in c:\\users\\ksang\\anaconda3\\lib\\site-packages (from plotly) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Installs plotly for displaying geospatial graphs\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "from copy import copy\n",
    "from scipy.stats import normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', \n",
    "          'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois','Indiana', 'Iowa', 'Kansas', \n",
    "          'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "          'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', \n",
    "          'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', \n",
    "          'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', \n",
    "          'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "states_abbv = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', \n",
    "               'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', \n",
    "               'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', \n",
    "               'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', \n",
    "               'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How ‘clean’ is the data?**\n",
    "\n",
    "Our data are clean in that they were provided by reputable sources who provided the data in a format without any unnecessary variables, null data, etc. For one of the datasets, some wage information was represented by a * or #, with the * meaning not enough data was available for inclusion, and the # meaning wage exceeded $200,000/yr\n",
    "\n",
    "**What did you have to do to get the data into a usable format?**\n",
    "\n",
    "Since some datasets were read in from excel datasheets, it was necessary to clean this data by removing the first few rows (title information), renaming the columns for proper identification, and resetting the index since the first n rows were removed. It was also necessary to remove additional columns that were not related to the dataframe, but were added for structural purposes in the excel datasheet.\n",
    "\n",
    "Some dataframes required transposing/reshaping in order to more easily work with the data.\n",
    "\n",
    "\n",
    "**What pre-processing steps were required for your methods?**\n",
    "\n",
    "For our state analysis, it was necessary to transpose our data in the beginning, since the variables in the original dataset were now to be used as observations in this new dataset. An additional transformation was made to “normalize” the number of employees in each column by dividing them by the total number of workers per state.\n",
    "\n",
    "We checked the distribution of variables such as probability of automation, wage, and employment percent change.\n",
    "\n",
    "For the data in Datasets 4, we needed to drop all non-total employee rows and merge the 9 datasets into one set.\n",
    "\n",
    "For the individual occupation wage analysis, it was necessary to drop most columns. We included occ code, annual mean wage, and occupation. The resulting dataset was merged with Dataset 1 by occ code so we could easily compare the likelihood of automation and the annual mean wage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function tidy-izes the data\n",
    "def organize(df, year):\n",
    "    '''\n",
    "    - Removes the first 7 columns from the dataframe (corresponds to title and additional non-data cells from\n",
    "    excel formatting)\n",
    "    - Adds column titles back in\n",
    "    - Drops null columns\n",
    "    - Trims dataframe to only include occupation and title\n",
    "    '''\n",
    "    df = df[7:]\n",
    "    df = df.rename(columns={df.columns[0]: 'Occupation', df.columns[1]: 'Total{}'.format(year)})\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df[['Occupation', 'Total{}'.format(year)]]\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pertinent DataFrames\n",
    "df_prob = pd.read_csv('datasets/raw_state_automation_data.csv', encoding='cp1252')\n",
    "df_employment = pd.read_excel('datasets/employmentbystate.xls')\n",
    "df_wages = pd.read_excel('datasets/wagedata.xlsx')\n",
    "unemployment = pd.read_csv('datasets/USunemployment.csv')\n",
    "economyData = pd.read_csv('datasets/economy-and-growth-indicators-for-united-states-1.csv')\n",
    "economyState = pd.read_csv('datasets/SAEMP25N__ALL_AREAS_1998_2018.csv')\n",
    "income = pd.read_csv('datasets/PARPI_PORT_2008_2017.csv')\n",
    "df_occ = pd.read_csv('datasets/raw_state_automation_data.csv', encoding='cp1252')\n",
    "\n",
    "\n",
    "employment2011 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2011.xlsx'), 2011)\n",
    "employment2012 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2012.xlsx'), 2012)\n",
    "employment2013 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2013.xlsx'), 2013)\n",
    "employment2014 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2014.xlsx'), 2014)\n",
    "employment2015 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2015.xlsx'), 2015)\n",
    "employment2016 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2016.xlsx'), 2016)\n",
    "employment2017 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2017.xlsx'), 2017)\n",
    "employment2018 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2018.xlsx'), 2018)\n",
    "employment2019 = organize(pd.read_excel('datasets/blsdata/cpsaat11b2019.xlsx'), 2019)\n",
    "\n",
    "# Merging the different employment datasets in order to analyze percent change\n",
    "employment = pd.merge(pd.merge(employment2011, employment2012), \\\n",
    "                      pd.merge(pd.merge(employment2013, pd.merge(employment2014, employment2015)), \\\n",
    "                               pd.merge(pd.merge(employment2016, employment2017), \\\n",
    "                                        pd.merge(employment2018, employment2019))))\n",
    "\n",
    "# Creating copy of main dataframe to avoid issues with individual cleaning and analyses\n",
    "df_prob_m = copy(df_prob)\n",
    "df_prob_g = copy(df_prob)\n",
    "df_prob_h = copy(df_prob)\n",
    "df_prob_k = copy(df_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment by Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure main dataset\n",
    "df_prob_m.sort_values(by=['SOC'], inplace=True)\n",
    "\n",
    "# Standardize Occupation column between datasets\n",
    "employment['Occupation'] = employment['Occupation'].apply(lambda x: x.title())\n",
    "df_prob_m['Occupation'] = df_prob_m['Occupation'].apply(lambda x: x.title())\n",
    "\n",
    "# Create trimmed dataset\n",
    "df_prob_m_trim = df_prob_m[['SOC', 'Occupation', 'Probability']]\n",
    "\n",
    "# Include employment info\n",
    "df_prob_m_trim = pd.merge(df_prob_m_trim, employment)\n",
    "\n",
    "# Add percent change based on 2012 to 2019 data (Note that data cannot be analyzed for 2011 as there were some\n",
    "## occupations that did not yet have employees, causing a division by 0 error)\n",
    "df_prob_m_trim['percent_change'] = (df_prob_m_trim.Total2019 - df_prob_m_trim.Total2012)/df_prob_m_trim.Total2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wage by Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary wage data\n",
    "df_wages = df_wages[['OCC_CODE', 'OCC_TITLE', 'A_MEAN']]\n",
    "df_wages.rename(columns={'OCC_CODE':'SOC', 'OCC_TITLE':'Occupation'},inplace=True)\n",
    "\n",
    "# Create combined probability & wages dataset\n",
    "df_probwages_m = pd.merge(df_prob_m, df_wages, how='left', left_on='SOC', right_on='SOC')\n",
    "\n",
    "# Drop rows if no mean wage info\n",
    "# * is used to represent occupation with insufficient data\n",
    "df_probwages_m = df_probwages_m[df_probwages_m.A_MEAN != '*']\n",
    "\n",
    "# Remove any null income values\n",
    "df_probwages_m.dropna(inplace=True,subset=['A_MEAN'])\n",
    "df_probwages_m = df_probwages_m.reset_index()\n",
    "\n",
    "# Rest of values should be numeric, so transform\n",
    "df_probwages_m.A_MEAN = pd.to_numeric(df_probwages_m.A_MEAN)\n",
    "\n",
    "# Add log mean income \n",
    "df_probwages_m['log_A_MEAN'] = df_probwages_m.A_MEAN.apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment & Automation by State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean employment data by removing excel specific titles & rows, dropping null data\n",
    "## adding proper column names, dropping unnecessary columns, and typecasting\n",
    "df_employment = df_employment[5:]\n",
    "df_employment.dropna(inplace=True)\n",
    "df_employment = df_employment.rename(columns={df_employment.columns[1]:'State', df_employment.columns[2]:'Employment'})\n",
    "df_employment.reset_index(inplace=True)\n",
    "df_employment = df_employment[['State', 'Employment']]\n",
    "df_employment.Employment = df_employment.Employment.astype(int)\n",
    "\n",
    "# Reshape data to easily apply later transformation\n",
    "df_employment = df_employment.transpose()\n",
    "df_employment.columns = df_employment.iloc[0]\n",
    "df_employment = df_employment.iloc[1:]\n",
    "\n",
    "# Transform employment data to reflect employment relative to population\n",
    "df_prob_m_normed = copy(df_prob_m)\n",
    "for state in states:\n",
    "    df_prob_m_normed[state] = df_prob_m_normed[state].apply(lambda x: x/df_employment[state])\n",
    "\n",
    "# Don't need SOC values, so remove\n",
    "#df_prob_m.drop(columns=['SOC'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wage by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = income[['GeoName', 'LineCode','2008', '2009', '2010', '2011', '2012', '2013', '2014' , '2015', '2016', '2017']]\n",
    "income = income[income.LineCode == 1.0]\n",
    "income = income.loc[income.index > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = income.reset_index()\n",
    "income = income.drop('index', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "totalIn = income.groupby(income.index // N).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalIn['change'] = totalIn['2017'] - totalIn['2008']\n",
    "totalIn['change'] = totalIn['change'] / totalIn['2008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want to look at probability, state data so remove other columns\n",
    "df_occ.drop(columns=['SOC', 'Occupation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform employment data to reflect employment relative to population\n",
    "for state in states:\n",
    "    df_occ[state] = df_occ[state].apply(lambda x: x/df_employment[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_likelihood2 = []\n",
    "for state in states:\n",
    "    likelihood2 = 0\n",
    "    for index in range(len(df_occ)):\n",
    "        likelihood2 += df_occ['Probability'][index] * df_occ[state][index]\n",
    "    state_likelihood2.append(likelihood2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statesDF = pd.DataFrame(states)\n",
    "totalIncomeState = totalIn.merge(statesDF, left_index = True, right_index = True)\n",
    "totalIncomeState = totalIncomeState.rename(columns={0: \"State\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoodAutomation = pd.DataFrame(state_likelihood2)\n",
    "incomeANDautomation = likelihoodAutomation.merge(totalIncomeState, left_index=True, right_index=True)\n",
    "incomeANDautomation = incomeANDautomation.rename(columns={0: \"Automation\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment by Occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributions**\n",
    "\n",
    "Our probability of automation variable is bound between 0-1 with a bimodal distribution. The peaks occur at the two boundaries. While the left boundary has a higher peak, the right mode carries more weight.\n",
    "\n",
    "Our variable representing the percent change in employment between 2011 and 2019 follows a relatively normal unimodal distribution with few mini-peaks that do not change the shape drastically. This distribution is slightly right-skewed.\n",
    "\n",
    "**Outliers**\n",
    "\n",
    "While there is a relatively smooth distribution outside of the boundary peaks, there is a non-modal peak in the 0.35-0.40 probability bin.\n",
    "\n",
    "There is a percent change outlier right around 4 (400% percent change). This corresponds to Transit And Railroad Police, which grew from 1,000 to 5,000 employees.\n",
    "\n",
    "**Relationship between variables**\n",
    "\n",
    "There is a very poor (horizontal) linear relation between the probability of automation and change in employment between 2012 and 2019. There is a stronger linear relationship between log annual income and probability of automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of probability of automation\n",
    "sns.distplot(df_prob_m.Probability, bins=20)\n",
    "plt.xlabel('Probability of Automation')\n",
    "plt.title('Distribution of Automation Probability', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of percent change in employment\n",
    "sns.distplot(df_prob_m_trim.percent_change)\n",
    "plt.xlabel('Change in Employment')\n",
    "plt.title('Distribution of Employment Change from 2012-2019', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate outlier\n",
    "df_prob_m_trim[df_prob_m_trim.percent_change >= 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine normality of percent_change\n",
    "k2, p = stats.normaltest(df_prob_m_trim.percent_change)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the percent change vs Probability of employment\n",
    "sns.scatterplot(df_prob_m_trim.Probability, df_prob_m_trim.percent_change)\n",
    "plt.xlabel('Probability of Automation')\n",
    "plt.ylabel('Change in Employment')\n",
    "plt.title('Probability of Automation vs Change in Employment', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What approaches did you use? Why?**\n",
    "\n",
    "We analyze the relationship between percent change in employment and probability of automation using an OLS Linear Regression model. This method of analysis was chosen because there is a relatively linear relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression for Probability and percent change\n",
    "# NOTE: Data does not meet requirements necessary for testing linearity, but want to take a look \n",
    "\n",
    "outcome, predictors = patsy.dmatrices('Probability ~ percent_change', df_prob_m_trim)\n",
    "model = sm.OLS(outcome, predictors)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What were the results?**\n",
    "\n",
    "The results from our analysis of the relationship between percent change in employment and the probability of automation shows inconclusive results. Due to the bimodal distribution of the probability of automation, our data was not properly distributed for this type of regression. Further attempts to normalize the data did not prove to be successful.\n",
    "\n",
    "**What were your interpretation of these findings?**\n",
    "\n",
    "Although the findings from the relation between employment percent change and automation did not prove to be conclusive of anything, we initially interpreted these results to show that the likelihood (probability) of automation is a complex factor with many contributing factors, and as such, we could not analyze it solely by looking at one possible contributor. This led to a breadth of comparisons being done in order to better understand the likelihood of automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wage by State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributions**\n",
    "\n",
    "**Outliers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(totalIn.change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(incomeANDautomation.Automation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(incomeANDautomation.Automation, incomeANDautomation.change)\n",
    "ax = sns.regplot(x=\"Automation\", y=\"change\", data=incomeANDautomation)\n",
    "ax.set(title='Job Automation Likelihood vs. Percent Change in Income per State', xlabel='Job Automation Likelihood', ylabel='Percent Change in Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome2, predictors2 = patsy.dmatrices('Automation ~ change', incomeANDautomation)\n",
    "model2 = sm.OLS(outcome2, predictors2)\n",
    "results2 = model2.fit()\n",
    "\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automation1of3 = incomeANDautomation.loc[incomeANDautomation.Automation <= 0.35]\n",
    "automation2of3 = incomeANDautomation.loc[(incomeANDautomation.Automation > 0.35) & (incomeANDautomation.Automation <= 0.4)]\n",
    "automation3of3 = incomeANDautomation.loc[incomeANDautomation.Automation > 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(automation3of3.Automation, automation3of3.change)\n",
    "ax3 = sns.regplot(x=\"Automation\", y=\"change\", data=automation3of3)\n",
    "ax3.set(xlabel='Automation Likelihood', ylabel='Percent Change in Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome3, predictors3 = patsy.dmatrices('Automation ~ change', automation2of3)\n",
    "model3 = sm.OLS(outcome3, predictors3)\n",
    "results3 = model3.fit()\n",
    "\n",
    "print(results3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(automation2of3.Automation, automation2of3.change)\n",
    "ax1 = sns.regplot(x=\"Automation\", y=\"change\", data=automation2of3)\n",
    "ax1.set(xlabel='Automation Likelihood', ylabel='Percent Change in Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome4, predictors4 = patsy.dmatrices('Automation ~ change', automation3of3)\n",
    "model4 = sm.OLS(outcome4, predictors4)\n",
    "results4 = model4.fit()\n",
    "\n",
    "print(results4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_incomeChange = []\n",
    "for state in states:\n",
    "    incomeChange = 0\n",
    "    for index in range(len(totalIncomeState)):\n",
    "        incomeChange = totalIncomeState['change'][index] \n",
    "        state_incomeChange.append(incomeChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=states_abbv,\n",
    "    z = state_incomeChange,\n",
    "    locationmode = 'USA-states',\n",
    "    colorscale = 'Purples',\n",
    "    colorbar_title =  \"Income Change\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Income Percent Change 2008 to 2017',\n",
    "    geo_scope='usa',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wage by Occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributions**\n",
    "\n",
    "Our annual mean wage variable takes a right-skewed normal distribution. We remove the skew by applying a natural log function to this data.\n",
    "\n",
    "**Outliers**\n",
    "\n",
    "There is at least one outlier in annual mean income around $225,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of annual mean income\n",
    "sns.distplot(df_probwages_m.A_MEAN)\n",
    "plt.xlabel('Mean Annual Income')\n",
    "plt.title('Distribution of Mean Annual Income', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate mean annual income outlier\n",
    "df_probwages_m[df_probwages_m.A_MEAN > 225000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these two extremely high paying outliers are both oral health occupations with a low probability of automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of log annual mean income\n",
    "sns.distplot(np.log(df_probwages_m.A_MEAN))\n",
    "plt.xlabel('Log(Mean Annual Income)')\n",
    "plt.title('Distribution of log Mean Annual Income', loc='left')\n",
    "plt.yticks(np.arange(0.0,1.2,0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normality of log wage\n",
    "stat_wage_mean_log, p_wage_mean_log = normaltest(df_probwages_m.log_A_MEAN)\n",
    "print('log mean wage is normally distributed') if p_wage_mean_log < 0.01 else print('log mean wage is NOT normally distributed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data for log mean wage are normally distributed, we can consider a linear regression model to analyze the relation between income and probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What approaches did you use? Why?**\n",
    "\n",
    "We analyze the relationship between log annual income and probability of automation by using an OLS Linear Regression model, because there is a clear linear relationship between these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete regression for relationship between probabilty of automation and log mean annual income\n",
    "outcome, predictors = patsy.dmatrices('Probability ~ log_A_MEAN', df_probwages_m)\n",
    "model = sm.OLS(outcome, predictors)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at ditribution of probability and log annual income\n",
    "sns.scatterplot(df_probwages_m.Probability, df_probwages_m.log_A_MEAN)\n",
    "plt.xlabel('Probability of Automation')\n",
    "plt.ylabel('Mean Annual Income')\n",
    "plt.title('Probability of Automation vs Mean Annual Income', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What were the results?**\n",
    "\n",
    "The results from our analysis of the relationship between log annual income and the probability of automation provide a $P|t|$ value of 0.000, with an Adjusted $R^2$ value of 0.330\n",
    "\n",
    "**What were your interpretation of these findings?**\n",
    "\n",
    "We interpret the findings between log annual income and probability of income to support our initial hypothesis that lower paying jobs are more likely to suffer from job automation. Although our $R^2$ value is low from the OLS Regression we performed, our 0.000 p value shows that this data is at least conclusive. We believe that further analysis with additional variables can improve our adjusted $R^2$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment & Automation by State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What approaches did you use? Why?**\n",
    "\n",
    "For analysing which jobs are most likely to most contribute to automation, we look at the total number of employees per occupation in each state, and take this as a percent of the total number of working employees in that state. That percentage is then multiplied by the probability of automation, and each state is summed across each occupation in that state. This gives the relative weighted likelihood of automation across each state. We further look at which occupation most contributes to this factor, and organize this data geospatially. We chose to analyze this data in this way because it gave a relative score that allows us to compare each state’s overall probability of automation, instead of having our data skewed by looking at absolute employment values. This method also gives insight on which jobs need further attention in our analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite likelihood of unemployment per state\n",
    "state_likelihood = []\n",
    "df_state_data = pd.DataFrame()\n",
    "for state in states:\n",
    "    likelihood = 0.0\n",
    "    max_likelihood = 0.0\n",
    "    for index in range(len(df_prob_m)):\n",
    "        new_likelihood = df_prob_m['Probability'][index] * df_prob_m_normed[state][index]\n",
    "        likelihood += new_likelihood\n",
    "        if  new_likelihood > max_likelihood:\n",
    "            max_likelihood = new_likelihood\n",
    "            df_state_data[state] = (df_prob_m.Occupation[index], df_prob_m[state][index], df_prob_m.SOC[index])\n",
    "         \n",
    "    state_likelihood.append(likelihood)\n",
    "    #print('state: {}\\n\\t {}'.format(state, likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform state data dataframe for easier manipulation\n",
    "df_state_data = df_state_data.transpose()\n",
    "df_state_data.rename(columns={0:'Occupation', 1:'Number', 2:'SOC'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datatype for number of employees\n",
    "df_state_data.Number = df_state_data.Number.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the occupations that most contribute to a state's weighted likelihood of automation, along with the\n",
    "## occupation's respective wage and number of states in which it is the top contributor\n",
    "for occupation in df_state_data.Occupation.unique():\n",
    "    SOC = df_state_data[df_state_data.Occupation == occupation].SOC.unique()[0]\n",
    "    wage = int(df_wages[df_wages.SOC == SOC].A_MEAN.values[0])\n",
    "    n_states = len(df_state_data[df_state_data.Occupation == occupation])\n",
    "    print('Occupation: {}\\n   Wage: {}\\n   Num States: {}'.format(occupation, wage, n_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that an the most overwhelming amount of jobs that will be lost due to automation will occur to retail salespeople. (This analysis looks at the amount of people affected multiplied by the probability of automation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add text column for hover-info on map\n",
    "df_state_data['text'] = df_state_data.index + '<br>' + \\\n",
    "'Most affected occupation: ' + df_state_data.Occupation + '<br>' + \\\n",
    "'Num employees of occupation in state: ' + df_state_data.Number\n",
    "\n",
    "# Plot the weighted likelihood of automation by state\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=states_abbv, # Spatial coordinates\n",
    "    z = state_likelihood, # Data to be color-coded\n",
    "    locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "    colorscale = 'blues',\n",
    "    colorbar_title =  \"Automation Probability\",\n",
    "    autocolorscale=False,\n",
    "    text = df_state_data.text\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Likelihood of Job Automation by State',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What were the results?**\n",
    "\n",
    "The results from our analysis between likelihood of automation by state show that there are only five different jobs across all 50 states + DC (51 total “areas”) that are the state’s highest contributors towards job automation. These occupations include cashiers, retail workers, administrative assistants, food prep & service workers, and office clerical workers. For 40 out of the 51 areas we analyze, the highest contributing occupation are retail workers. This occupation has an average US wage of around $27,000, which is less than half of the average income for 2016 in which this data were collected, and falls below the first quartile for US workers. We also see that the lowest aggregate probability of automation is in DC, at 29.93\\% total risk of automation. The highest contributor in DC is secretaries.\n",
    "\n",
    "**What were your interpretation of these findings?**\n",
    "\n",
    "We interpret the findings of our state-automation data to support our initial hypothesis that lower-paying jobs will be more susceptible to job automation than higher paying jobs would. Since this statistic is based on the number of employees per occupation, it is necessary to make the distinction that this metric looks at the probability of automation for the highest number of jobs. A job with 100 employees and a .99 probability of automation would have a lower metric than a job with 200 employees and a .99 probability of automation. It is also interesting to consider secretaries in DC. The secretary position is the highest paying occupation from the five highest-contributor occupations. We interpret this under the assumption that a secretary is a relatively common job in DC (with each of the many political positions, among others, requiring at least one secretary). This means that secretaries make up a significant amount of the jobs held in DC, but current technological advancements like digital assistants and phone screenings may soon be capable of automating this position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our datasets have been provided by publicly available sources such as data.world, The Bureau of Labor Statistics, or other US Government agencies. Since these data are being provided to the public, we anticipate no restrictions in using it for the purpose of this project. Additionally, no restrictions have been posted for the datasets we are accessing. Much of the data we are using is provided by state or federal governments, which is required by law to provide strong protection for the data that are available to the public. For the data sources that are not guaranteed to do this, we will provide privacy by anonymizing any potentially personal identifiable information. However, we do not anticipate this being an issue, as all the data collected are aggregates that don’t include any potentially personal identifiable information. Occupations with less than 1,000 employees in a certain state were removed from our data.\n",
    "\n",
    "One of our dataset sources, data.world, consists of contributors of various backgrounds and experience levels. This increases the potential for bias, since there is no way of confirming whether the presentation of the data is biased towards fulfilling the contributor’s needs. The user has the option of viewing the contributor’s Kaggle profile and their LinkedIn profiles, but the amount of information that is provided to the user is controlled by the contributor themselves. If we utilize the provided information to confirm the legitimacy of the contributor’s data, we can decrease the chance of potential bias. For our other data source, data.gov, there is a smaller potential of bias given the fact that it is a government source. While we cannot assume that government sources are entirely free of bias, we can assume that it is more of a fair source than non-government sources. Government sources are supposed to be free of affiliation to political parties, which is why we can assume that the data presented has relatively low levels of bias.\n",
    "\n",
    "A potential ethical concern we have considered is data misinformation/misinterpretation. If an individual comes across our analysis without the understanding that this data relies partly on probability instead of concrete metrics, and that this analysis was not conducted by professional Data Scientists, it might lead to the unnecessary spread of fear that one might lose their job to automation, or further contribute to data misinterpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your discussion information here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Specify who in your group worked on which parts of the project.*\n",
    "\n",
    "Michael:\n",
    "- Setup/cleaning/eda/analysis/viz:\n",
    "    - Employment by Automation\n",
    "    - Wage by Occupation\n",
    "    - Employment & Automation by State\n",
    "- Datasets 1-4\n",
    "- Ethics & Privacy \n",
    "- Prior Work & Hypothesis\n",
    "\n",
    "Kiran:\n",
    "- Setup/cleaning/eda/analysis/viz:\n",
    "    - Gross Average Unemployment vs. Income United States\n",
    "    - Wage by State Income Change\n",
    "    - Analysis of Automation by State vs. Income Change per State\n",
    "        -Used Michael's code for automation probability generation \n",
    "- Datasets 5-8\n",
    "- Ethics & Privacy \n",
    "- Code for boxplots used throughout project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
